{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e0cb5ea",
   "metadata": {},
   "source": [
    "\n",
    "### CIFAR-10 DATASET\n",
    "60000 images of 32x32 color images in 10 classes.\n",
    "There are 50000 training images and 10000 test images.\n",
    "\n",
    "The classes are airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86bcfd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70097a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f4f5c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.22.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0c1d51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "694486da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Â device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e88b3af",
   "metadata": {},
   "source": [
    "## Set the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9b9324e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1e65b4",
   "metadata": {},
   "source": [
    "## Setting the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "776f96fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 3e-4\n",
    "PATCH_SIZE = 4\n",
    "NUM_CLASSES = 10\n",
    "IMAGE_SIZE = 32\n",
    "CHANNELS = 3\n",
    "EMBED_DIM = 256\n",
    "NUM_HEADS = 8 # number of attention heads\n",
    "DEPTH = 6 # number of transformer blocks\n",
    "MLP_DIM = 512 # dimension of the feedforward network\n",
    "DROP_RATE = 0.1 # dropout rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd02c4cb",
   "metadata": {},
   "source": [
    "## Define Image Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6a75ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # Convert image to tensor\n",
    "    transforms.Normalize((0.5), (0.5)), # Normalize tensor: 1. Helps the model converge faster; 2. Helps to make numerical computation stable\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06a96563",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.CIFAR10(root='./data',\n",
    "                                 train=True,\n",
    "                                 download=True,\n",
    "                                 transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8303fcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = datasets.CIFAR10(root='./data',\n",
    "                                train=False,\n",
    "                                download=True,\n",
    "                                transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c5aeb6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset CIFAR10\n",
       "     Number of datapoints: 50000\n",
       "     Root location: ./data\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "                Normalize(mean=0.5, std=0.5)\n",
       "            ),\n",
       " 50000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00b52551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset CIFAR10\n",
       "     Number of datapoints: 10000\n",
       "     Root location: ./data\n",
       "     Split: Test\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "                Normalize(mean=0.5, std=0.5)\n",
       "            ),\n",
       " 10000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset, len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8079784b",
   "metadata": {},
   "source": [
    "## Converting datasets to DataLoader\n",
    "Right now our data is in the form of PyTorch datasets. To feed this data into our model, we need to convert it into a DataLoader, which will allow us to easily iterate over the data in batches (DataLoader turns data into batches).\n",
    "\n",
    "We need batches because our model processes data in parallel, and having a batch of samples allows for more efficient computation and better utilization of hardware resources, which otherwise could not fit an entire dataset into memory.\n",
    "\n",
    "The neural network will update its weights based on the average loss over the entire batch, rather than on individual samples. This leads to more stable and efficient training.\n",
    "\n",
    "Let's create DataLoaders for our training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cc16fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "681af9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader: <torch.utils.data.dataloader.DataLoader object at 0x15779cd60>, <torch.utils.data.dataloader.DataLoader object at 0x111761220>\n",
      "Length: 391 and 79 batches of size 128\n"
     ]
    }
   ],
   "source": [
    "print(f\"DataLoader: {train_loader}, {test_loader}\")\n",
    "print(f\"Length: {len(train_loader)} and {len(test_loader)} batches of size {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa40c105",
   "metadata": {},
   "source": [
    "Note that we now have 391*128 images in total, which is 50048, and bigger than our original dataset size of 50000 images. This is because DataLoader fills the last batch with duplicate samples to reach the desired batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c435e5cd",
   "metadata": {},
   "source": [
    "## Building Vision Transformer Model From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a88085c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split images into patches\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self,\n",
    "                img_size,\n",
    "                patch_size,\n",
    "                in_channels,\n",
    "                embed_dim):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        # define the projection layer\n",
    "        self.proj = nn.Conv2d(in_channels=in_channels,\n",
    "                              out_channels=embed_dim,\n",
    "                              kernel_size=patch_size,\n",
    "                              stride=patch_size)\n",
    "        num_patches = (img_size // patch_size) ** 2 # ** 2 because we have a square image\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim)) # class token\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 1 + num_patches, embed_dim)) # position embedding\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, C, H, W = x.shape\n",
    "        # convert image to patches\n",
    "        x = self.proj(x) # (B, embed_dim, H/patch_size, W/patch_size)\n",
    "        # flatten(2) flattens the last two dimensions: (H/patch_size * W/patch_size = num_patches)\n",
    "        x = x.flatten(2).transpose(1, 2) # (B, num_patches, embed_dim) (transpose(1,2) swaps the first and second dimensions)\n",
    "        # expand the class token to the batch size. -1,-1 is used to keep the size of the other dimensions the same\n",
    "        cls_token = self.cls_token.expand(B, -1, -1) # (B, 1, embed_dim)\n",
    "        # concatenate the class token and the patch embeddings along the sequence dimension\n",
    "        x = torch.cat((cls_token, x), dim=1) # (B, 1 + num_patches, embed_dim)\n",
    "        x = x + self.pos_embed # (B, 1 + num_patches, embed_dim)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fbc0efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 hidden_features,\n",
    "                 drop_rate):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=in_features, out_features=hidden_features)\n",
    "        self.fc2 = nn.Linear(in_features=hidden_features, out_features=in_features) # in_features because we want to project back to the original dimension\n",
    "        self.dropout = nn.Dropout(drop_rate) # randomly zero out some of the features with probability drop_rate\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.dropout(F.gelu(self.fc1(x))) # GELU activation function\n",
    "        x = self.dropout(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41266328",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim,\n",
    "                 num_heads,\n",
    "                 mlp_dim,\n",
    "                 drop_rate):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim) # layer normalization\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=drop_rate) # multi-head self-attention\n",
    "        self.norm2 = nn.LayerNorm(embed_dim) # layer normalization\n",
    "        self.mlp = MLP(embed_dim, mlp_dim, drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0] # residual connection\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc4bf495",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size,\n",
    "                 patch_size,\n",
    "                 in_channels,\n",
    "                 num_classes,\n",
    "                 embed_dim,\n",
    "                 depth,\n",
    "                 num_heads,\n",
    "                 mlp_dim,\n",
    "                 drop_rate):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        self.encoder = nn.Sequential(\n",
    "            # we use * to unpack the list of layers and pass it to the Sequential container\n",
    "            *[TransformerEncoderLayer(embed_dim, num_heads, mlp_dim, drop_rate) for _ in range(depth)]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes) # output layer for classification\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.encoder(x)\n",
    "        x = self.norm(x)\n",
    "        # take the class token\n",
    "        cls_token = x[:, 0] # [:, 0] because it's the first token of the sequence\n",
    "        x = self.head(cls_token)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1159d155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "model = VisionTransformer(\n",
    "    img_size=IMAGE_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    in_channels=CHANNELS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    depth=DEPTH,\n",
    "    num_heads=NUM_HEADS,\n",
    "    mlp_dim=MLP_DIM,\n",
    "    drop_rate=DROP_RATE\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f4c104d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbedding(\n",
       "    (proj): Conv2d(3, 256, kernel_size=(4, 4), stride=(4, 4))\n",
       "  )\n",
       "  (encoder): Sequential(\n",
       "    (0): TransformerEncoderLayer(\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerEncoderLayer(\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerEncoderLayer(\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerEncoderLayer(\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerEncoderLayer(\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerEncoderLayer(\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a1df69",
   "metadata": {},
   "source": [
    "## Defining a Loss function and an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbdb5fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CrossEntropyLoss(),\n",
       " Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     decoupled_weight_decay: False\n",
       "     differentiable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     fused: None\n",
       "     lr: 0.0003\n",
       "     maximize: False\n",
       "     weight_decay: 0\n",
       " ))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss() # for multi-class classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "criterion, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08684b75",
   "metadata": {},
   "source": [
    "## Defining a Training Loop function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7bcbc8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "\n",
    "    total_loss, correct = 0, 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        # Moving data to the device\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad() # zero the gradients\n",
    "        # 1. Forward pass (model outputs raw logits)\n",
    "        out = model(x)\n",
    "        # 2. Calculate loss\n",
    "        loss = criterion(out, y)\n",
    "        # 3. Perform backpropagation\n",
    "        loss.backward()\n",
    "        # 4. Perform Gradient Descent (Update the weights)\n",
    "        optimizer.step()\n",
    "        # accumulate training loss (CEL returns the average over a batch)\n",
    "        total_loss += loss.item() * x.size(0) # Multiply by x.size(0) to get an average over all samples\n",
    "        correct += (out.argmax(dim=1) == y).sum().item() # count correct predictions\n",
    "    # we scale the loss to account for the batch size\n",
    "    return total_loss / len(loader.dataset), correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e0f6a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.inference_mode(): # Disable gradient tracking\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            correct += (out.argmax(dim=1) == y).sum().item()\n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad94b909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using Colab\n",
    "# from tqdm.auto import tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a651c0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "### Training\n",
    "train_accuracies, test_accuracies = [], []\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    test_acc = evaluate(model, test_loader)\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Train Loss: {train_loss:.4f} - Train Acc: {train_acc:.4f}% - Test Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48b6db0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
