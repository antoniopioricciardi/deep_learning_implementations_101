{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07f13009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dcae175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 20, 20]), torch.Size([10, 20, 20]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch, seq_len, embedding_dim = 10, 20, 30  # Example dimensions\n",
    "x = torch.randn(batch, seq_len, embedding_dim)  # Example tensor of shape (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "raw_weights = torch.bmm(x, x.transpose(1, 2))  # Compute raw attention weights\n",
    "normalized_weights = F.softmax(raw_weights, dim=-1)  # Normalize weights using softmax\n",
    "\n",
    "raw_weights.shape, normalized_weights.shape  # Check shapes of the computed weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03af8d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.bmm(normalized_weights, x) # Apply attention weights to the input tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83e64d5",
   "metadata": {},
   "source": [
    "This is all we need to get self-attention working in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb3b791",
   "metadata": {},
   "source": [
    "# Additional tricks\n",
    "Actual self-attention relies on three additional tricks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09bf261",
   "metadata": {},
   "source": [
    "### 1. Query, Key, Value\n",
    "In self-attention, we compute three different representations of the input: Query, Key, and Value. These are derived from the input tensor and are used to compute attention weights.\n",
    "\n",
    "We can implement this by defining three linear transformations for the input tensor.\n",
    "We will use PyTorch's `nn.Linear` to create these transformations, namely:\n",
    "$W_q$ for Query, $W_k$ for Key, and $W_v$ for Value.\n",
    "\n",
    "We then compute the raw attention weights by taking the dot product of the Query and Key matrices, followed by a softmax operation to normalize the weights:\n",
    "\n",
    "$q_i = W_q(x_i), \\qquad k_i = W_k(x_i), \\qquad v_i = W_v(x_i)$\n",
    "\n",
    "Then, the raw attention weights are computed as:\n",
    "$ w'_{ij} = q_i ^T \\cdot k_j $\n",
    "\n",
    "We then normalize these weights using softmax:\n",
    "$ w_{ij} = \\text{softmax}(w'_{ij}) $\n",
    "\n",
    "Finally, we apply the attention weights to the Value matrix:\n",
    "$ y_i = \\sum_j w_{ij} v_j $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6df000f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_q = torch.randn(embedding_dim, embedding_dim)  # Query weight matrix\n",
    "W_k = torch.randn(embedding_dim, embedding_dim)  # Key weight matrix\n",
    "W_v = torch.randn(embedding_dim, embedding_dim)  # Value weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b869b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 30, 30]), torch.Size([10, 30, 30]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_q.unsqueeze(0).shape, W_q.unsqueeze(0).expand(batch, -1, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e55b61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.bmm(x, W_q.unsqueeze(0).expand(batch, -1, -1))  # Compute queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea3583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, k, num_heads=1, mask=False):\n",
    "        super.__init__()\n",
    "        assert k % num_heads == 0, \"k must be divisible by num_heads\"\n",
    "        self.k, self.num_heads = k, num_heads\n",
    "\n",
    "        # These compute the queries, keys and values for all\n",
    "        # heads\n",
    "        self.W_q = torch.nn.Linear(k, k, bias=False)\n",
    "        self.W_k = torch.nn.Linear(k, k, bias=False)\n",
    "        self.W_v = torch.nn.Linear(k, k, bias=False)\n",
    "\n",
    "        # Applied after the multi-head self-attention operation\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, k = x.size()\n",
    "        h = self.num_heads\n",
    "\n",
    "        queries = self.W_q(x)\n",
    "        keys = self.W_k(x)\n",
    "        values = self.W_v(x)\n",
    "\n",
    "        # to divide queries, keys, values into chunks\n",
    "        s = k//h\n",
    "\n",
    "        queries = queries.view(b, t, h, s)\n",
    "        keys = keys.view(b, t, h, s)\n",
    "        values = values.view(b, t, h, s)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
