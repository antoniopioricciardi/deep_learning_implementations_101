{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbda7c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8dcbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, k, num_heads=1, mask=False):\n",
    "        super().__init__()\n",
    "        assert k % num_heads == 0, \"k must be divisible by num_heads\"\n",
    "        self.k, self.num_heads = k, num_heads\n",
    "\n",
    "        # These compute the queries, keys and values for all\n",
    "        # heads\n",
    "        self.W_q = torch.nn.Linear(k, k, bias=False)\n",
    "        self.W_k = torch.nn.Linear(k, k, bias=False)\n",
    "        self.W_v = torch.nn.Linear(k, k, bias=False)\n",
    "\n",
    "        # Applied after the multi-head self-attention operation\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, k = x.size()\n",
    "        h = self.num_heads\n",
    "\n",
    "        queries = self.W_q(x)\n",
    "        keys = self.W_k(x)\n",
    "        values = self.W_v(x)\n",
    "        self.unify_heads = nn.Linear(k, k)\n",
    "\n",
    "        # to divide queries, keys, values into chunks\n",
    "        s = k//h\n",
    "\n",
    "        queries = queries.view(b, t, h, s)\n",
    "        keys = keys.view(b, t, h, s)\n",
    "        values = values.view(b, t, h, s)\n",
    "\n",
    "        print(f\"queries: {queries.shape}, keys: {keys.shape}, values: {values.shape}\")\n",
    "\n",
    "        print(keys.transpose(1, 2).contiguous().view(b*h, t, s).shape)\n",
    "\n",
    "        # fold heads into the batch dimension\n",
    "        # Transpose to get the shape (b, h, t, s), then view to (b*h, t, s)\n",
    "\n",
    "        queries = queries.transpose(1, 2).contiguous().view(b*h, t, s)\n",
    "        keys = keys.transpose(1, 2).contiguous().view(b*h, t, s)\n",
    "        values = values.transpose(1, 2).contiguous().view(b*h, t, s)\n",
    "\n",
    "        # when we compute wij = q_i * k_j^T, we get a single number for each pair of i, j\n",
    "        # so we need to compute the dot product for all pairs of queries and keys\n",
    "        # this gives us a matrix of shape (b*h, t, t) where each row is a query and each column is a key\n",
    "\n",
    "        # Get dot product of queries and keys, and scale\n",
    "        dot = torch.bmm(queries, keys.transpose(1, 2))  # (b*h, t, t)\n",
    "        # -- dot has size (b*h, t, t) containing raw weights\n",
    "        dot = dot / (s ** 0.5)  # Scale by sqrt(d_k)\n",
    "\n",
    "        # normalize\n",
    "        dot = F.softmax(dot, dim=2)\n",
    "        # - dot now contains row-wise normalized weights\n",
    "        \n",
    "        # apply the self attention to the values\n",
    "        # dot has size (b*h, t, t) and values has size (b*h, t, s)\n",
    "        out = torch.bmm(dot, values).view(b, h, t, s) # (b, h, t, s)\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous().view(b, t, s*h)  # (b, t, k)\n",
    "\n",
    "        # unify heads\n",
    "        # out has size (b, t, k) where k = s * h\n",
    "        # we need to unify the heads into a single dimension\n",
    "        # this is done by a linear layer that maps (b, t, k) to (b, t, k)\n",
    "        # where k is the original embedding dimension\n",
    "        out = self.unify_heads(out)  # (b, t, k)\n",
    "        \n",
    "        return out  # (b, t, k) - the output of the self-attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be34b0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queries: torch.Size([10, 20, 4, 8]), keys: torch.Size([10, 20, 4, 8]), values: torch.Size([10, 20, 4, 8])\n",
      "torch.Size([40, 20, 8])\n",
      "out: torch.Size([10, 20, 32])\n",
      "unifying heads: torch.Size([10, 20, 32])\n"
     ]
    }
   ],
   "source": [
    "batch, seq_len, embedding_dim = 10, 20, 32  # Example dimensions\n",
    "x = torch.randn(batch, seq_len, embedding_dim)  # Example input tensor\n",
    "\n",
    "self_attention = SelfAttention(k=embedding_dim, num_heads=4, mask=False)\n",
    "output = self_attention(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46072129",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, k, heads):\n",
    "        super().__init__()\n",
    "        self.self_attention = SelfAttention(k, heads=heads)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(k)\n",
    "        self.norm2 = nn.LayerNorm(k)\n",
    "\n",
    "        # Feed-forward network, expansion should always be bigger than the input/output layers.\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(k, 4*k),  # Feed-forward layer with expansion (4 times the input size, arbitrary choice)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*k, k)  # Output layer\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attended = self.self_attention(x)\n",
    "        x = self.norm1(attended + x)  # Add & Norm\n",
    "        feedforward = self.ff(x)\n",
    "        x = self.norm2(feedforward + x)  # Add & Norm\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
